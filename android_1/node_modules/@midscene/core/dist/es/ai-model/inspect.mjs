import { generateElementByPosition } from "@midscene/shared/extractor/dom-util";
import { cropByRect, paddingToMatchBlockByBase64, preProcessImageUrl } from "@midscene/shared/img";
import { getDebug } from "@midscene/shared/logger";
import { assert } from "@midscene/shared/utils";
import { AIActionType, adaptBboxToRect, expandSearchArea, mergeRects } from "../common.mjs";
import { extractDataQueryPrompt, systemPromptToExtract } from "./prompt/extraction.mjs";
import { findElementPrompt, systemPromptToLocateElement } from "./prompt/llm-locator.mjs";
import { sectionLocatorInstruction, systemPromptToLocateSection } from "./prompt/llm-section-locator.mjs";
import { orderSensitiveJudgePrompt, systemPromptToJudgeOrderSensitive } from "./prompt/order-sensitive-judge.mjs";
import { callAIWithObjectResponse } from "./service-caller/index.mjs";
const debugInspect = getDebug('ai:inspect');
const debugSection = getDebug('ai:section');
const extraTextFromUserPrompt = (prompt)=>{
    if ('string' == typeof prompt) return prompt;
    return prompt.prompt;
};
const promptsToChatParam = async (multimodalPrompt)=>{
    const msgs = [];
    if (multimodalPrompt?.images?.length) {
        msgs.push({
            role: 'user',
            content: [
                {
                    type: 'text',
                    text: 'Next, I will provide all the reference images.'
                }
            ]
        });
        for (const item of multimodalPrompt.images){
            const base64 = await preProcessImageUrl(item.url, !!multimodalPrompt.convertHttpImage2Base64);
            msgs.push({
                role: 'user',
                content: [
                    {
                        type: 'text',
                        text: `this is the reference image named '${item.name}':`
                    }
                ]
            });
            msgs.push({
                role: 'user',
                content: [
                    {
                        type: 'image_url',
                        image_url: {
                            url: base64,
                            detail: 'high'
                        }
                    }
                ]
            });
        }
    }
    return msgs;
};
async function AiLocateElement(options) {
    const { context, targetElementDescription, callAIFn, modelConfig } = options;
    const { vlMode } = modelConfig;
    const { screenshotBase64 } = context;
    assert(targetElementDescription, "cannot find the target element description");
    const targetElementDescriptionText = extraTextFromUserPrompt(targetElementDescription);
    const userInstructionPrompt = findElementPrompt(targetElementDescriptionText);
    const systemPrompt = systemPromptToLocateElement(vlMode);
    let imagePayload = screenshotBase64;
    let imageWidth = context.size.width;
    let imageHeight = context.size.height;
    let originalImageWidth = imageWidth;
    let originalImageHeight = imageHeight;
    if (options.searchConfig) {
        assert(options.searchConfig.rect, 'searchArea is provided but its rect cannot be found. Failed to locate element');
        assert(options.searchConfig.imageBase64, 'searchArea is provided but its imageBase64 cannot be found. Failed to locate element');
        imagePayload = options.searchConfig.imageBase64;
        imageWidth = options.searchConfig.rect?.width;
        imageHeight = options.searchConfig.rect?.height;
        originalImageWidth = imageWidth;
        originalImageHeight = imageHeight;
    } else if ('qwen2.5-vl' === vlMode) {
        const paddedResult = await paddingToMatchBlockByBase64(imagePayload);
        imageWidth = paddedResult.width;
        imageHeight = paddedResult.height;
        imagePayload = paddedResult.imageBase64;
    }
    const msgs = [
        {
            role: 'system',
            content: systemPrompt
        },
        {
            role: 'user',
            content: [
                {
                    type: 'image_url',
                    image_url: {
                        url: imagePayload,
                        detail: 'high'
                    }
                },
                {
                    type: 'text',
                    text: userInstructionPrompt
                }
            ]
        }
    ];
    if ('string' != typeof targetElementDescription) {
        const addOns = await promptsToChatParam({
            images: targetElementDescription.images,
            convertHttpImage2Base64: targetElementDescription.convertHttpImage2Base64
        });
        msgs.push(...addOns);
    }
    const res = await callAIFn(msgs, AIActionType.INSPECT_ELEMENT, modelConfig);
    const rawResponse = JSON.stringify(res.content);
    let resRect;
    let matchedElements = 'elements' in res.content ? res.content.elements : [];
    let errors = 'errors' in res.content ? res.content.errors : [];
    try {
        if ('bbox' in res.content && Array.isArray(res.content.bbox) && res.content.bbox.length >= 1) {
            resRect = adaptBboxToRect(res.content.bbox, imageWidth, imageHeight, options.searchConfig?.rect?.left, options.searchConfig?.rect?.top, originalImageWidth, originalImageHeight, vlMode);
            debugInspect('resRect', resRect);
            const rectCenter = {
                x: resRect.left + resRect.width / 2,
                y: resRect.top + resRect.height / 2
            };
            const element = generateElementByPosition(rectCenter, targetElementDescriptionText);
            errors = [];
            if (element) matchedElements = [
                element
            ];
        }
    } catch (e) {
        const msg = e instanceof Error ? `Failed to parse bbox: ${e.message}` : 'unknown error in locate';
        if (errors && errors?.length !== 0) errors.push(`(${msg})`);
        else errors = [
            msg
        ];
    }
    return {
        rect: resRect,
        parseResult: {
            elements: matchedElements,
            errors: errors
        },
        rawResponse,
        usage: res.usage
    };
}
async function AiLocateSection(options) {
    const { context, sectionDescription, modelConfig } = options;
    const { vlMode } = modelConfig;
    const { screenshotBase64 } = context;
    const systemPrompt = systemPromptToLocateSection(vlMode);
    const sectionLocatorInstructionText = sectionLocatorInstruction(extraTextFromUserPrompt(sectionDescription));
    const msgs = [
        {
            role: 'system',
            content: systemPrompt
        },
        {
            role: 'user',
            content: [
                {
                    type: 'image_url',
                    image_url: {
                        url: screenshotBase64,
                        detail: 'high'
                    }
                },
                {
                    type: 'text',
                    text: sectionLocatorInstructionText
                }
            ]
        }
    ];
    if ('string' != typeof sectionDescription) {
        const addOns = await promptsToChatParam({
            images: sectionDescription.images,
            convertHttpImage2Base64: sectionDescription.convertHttpImage2Base64
        });
        msgs.push(...addOns);
    }
    const result = await callAIWithObjectResponse(msgs, AIActionType.EXTRACT_DATA, modelConfig);
    let sectionRect;
    const sectionBbox = result.content.bbox;
    if (sectionBbox) {
        const targetRect = adaptBboxToRect(sectionBbox, context.size.width, context.size.height, 0, 0, context.size.width, context.size.height, vlMode);
        debugSection('original targetRect %j', targetRect);
        const referenceBboxList = result.content.references_bbox || [];
        debugSection('referenceBboxList %j', referenceBboxList);
        const referenceRects = referenceBboxList.filter((bbox)=>Array.isArray(bbox)).map((bbox)=>adaptBboxToRect(bbox, context.size.width, context.size.height, 0, 0, context.size.width, context.size.height, vlMode));
        debugSection('referenceRects %j', referenceRects);
        const mergedRect = mergeRects([
            targetRect,
            ...referenceRects
        ]);
        debugSection('mergedRect %j', mergedRect);
        sectionRect = expandSearchArea(mergedRect, context.size, vlMode);
        debugSection('expanded sectionRect %j', sectionRect);
    }
    let imageBase64 = screenshotBase64;
    if (sectionRect) {
        const croppedResult = await cropByRect(screenshotBase64, sectionRect, 'qwen2.5-vl' === vlMode);
        imageBase64 = croppedResult.imageBase64;
        sectionRect.width = croppedResult.width;
        sectionRect.height = croppedResult.height;
    }
    return {
        rect: sectionRect,
        imageBase64,
        error: result.content.error,
        rawResponse: JSON.stringify(result.content),
        usage: result.usage
    };
}
async function AiExtractElementInfo(options) {
    const { dataQuery, context, extractOption, multimodalPrompt, modelConfig } = options;
    const systemPrompt = systemPromptToExtract();
    const { screenshotBase64 } = context;
    const extractDataPromptText = extractDataQueryPrompt(options.pageDescription || '', dataQuery);
    const userContent = [];
    if (extractOption?.screenshotIncluded !== false) userContent.push({
        type: 'image_url',
        image_url: {
            url: screenshotBase64,
            detail: 'high'
        }
    });
    userContent.push({
        type: 'text',
        text: extractDataPromptText
    });
    const msgs = [
        {
            role: 'system',
            content: systemPrompt
        },
        {
            role: 'user',
            content: userContent
        }
    ];
    if (multimodalPrompt) {
        const addOns = await promptsToChatParam({
            images: multimodalPrompt.images,
            convertHttpImage2Base64: multimodalPrompt.convertHttpImage2Base64
        });
        msgs.push(...addOns);
    }
    const result = await callAIWithObjectResponse(msgs, AIActionType.EXTRACT_DATA, modelConfig);
    return {
        parseResult: result.content,
        usage: result.usage
    };
}
async function AiJudgeOrderSensitive(description, callAIFn, modelConfig) {
    const systemPrompt = systemPromptToJudgeOrderSensitive();
    const userPrompt = orderSensitiveJudgePrompt(description);
    const msgs = [
        {
            role: 'system',
            content: systemPrompt
        },
        {
            role: 'user',
            content: userPrompt
        }
    ];
    const result = await callAIFn(msgs, AIActionType.INSPECT_ELEMENT, modelConfig);
    return {
        isOrderSensitive: result.content.isOrderSensitive ?? false,
        usage: result.usage
    };
}
export { AiExtractElementInfo, AiJudgeOrderSensitive, AiLocateElement, AiLocateSection };

//# sourceMappingURL=inspect.mjs.map